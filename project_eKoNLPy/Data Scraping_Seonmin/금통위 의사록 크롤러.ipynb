{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n",
      "finish...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "pdf_list = []\n",
    "\n",
    "\n",
    "# 3~31까지\n",
    "for i in range(3,30):\n",
    "    url = 'http://www.bok.or.kr/portal/bbs/B0000245/list.do?menuNo=200761&pageIndex='+str(i)\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    \n",
    "    table = soup.find('div', class_='bdLine type2')\n",
    "    span_1_list = table.find_all('span',class_='col m10 s10 x9 ctBx')\n",
    "    span_2_list = table.find_all('div',class_='col s12 dataInfo')\n",
    "    span_3_list = table.find_all('div', class_='col m2 s2 x3 fileLink')\n",
    "\n",
    "    title_list = []\n",
    "    date_list = []\n",
    "    link_list = []\n",
    "\n",
    "    for span_1 in span_1_list:\n",
    "        title = span_1.find('span', class_='titlesub').text\n",
    "        title_list.append(title)\n",
    "\n",
    "    for span_2 in span_2_list:\n",
    "        date = span_2.find('span', class_='date').text.replace('등록일','')\n",
    "        date_list.append(date)\n",
    "        \n",
    "        \n",
    "\n",
    "    for span_3 in span_3_list:\n",
    "        classifi = span_3.find_all('li')[0].find('a')['title']\n",
    "\n",
    "        if classifi[-3:] == 'hwp':\n",
    "            pdf_link =span_3.find_all('li')[1].find('a')['href']\n",
    "            link_list.append(pdf_link)\n",
    "        else:\n",
    "            pdf_link =span_3.find_all('li')[0].find('a')['href']\n",
    "            link_list.append(pdf_link)\n",
    "\n",
    "    names_urls = zip(title_list, date_list, link_list)\n",
    "#         print(names_urls)\n",
    "\n",
    "    base_pdf = 'http://www.bok.or.kr'\n",
    "\n",
    "    for name, date, url in names_urls:\n",
    "        name = date +' '+ name +'.pdf'\n",
    "        pdf_list.append(name)\n",
    "        url = base_pdf + url\n",
    "        rq = urllib.request.Request(url)\n",
    "        res = urllib.request.urlopen(rq).read()\n",
    "        with open(name, mode=\"wb\") as f:\n",
    "            f.write(res)\n",
    "            print(\"finish...\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF to txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 갓티카\n",
    "\n",
    "tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tika import parser #https://github.com/chrismattmann/tika-python\n",
    "import shutil\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "# 폴더내 pdf파일을 txt 파일로 변경\n",
    "# source_folder는 pdf저장된 폴더경로, output_folder는 txt저장할 폴더경로\n",
    "def pdf2txt (source_folder=\"./data/bondreports/pdf/\", output_folder=\"./data/bondreports/text/\") :\n",
    "    # 지정 폴더 내 파일 목록 조회 (파일만)\n",
    "    pdf_files = [f for f in listdir(source_folder) if isfile(join(source_folder, f))]\n",
    "    \n",
    "    for pdf in pdf_files :\n",
    "        pdf_filepath = source_folder + pdf\n",
    "        pdf_tmp_filepath = output_folder + 'tmp.pdf'\n",
    "        # pdf 파일을 text로 변환\n",
    "\n",
    "        \n",
    "        shutil.copyfile(pdf_filepath, pdf_tmp_filepath)\n",
    "            \n",
    "        parsedPDF = re.sub('\\n', '', parser.from_file(pdf_tmp_filepath)[\"content\"])\n",
    "        #print(parsedPDF)\n",
    "\n",
    "        ouput_filepath = (output_folder + pdf).replace('.pdf', '.txt')\n",
    "        with open(ouput_filepath, 'w', encoding='utf8' ) as f :\n",
    "            print(ouput_filepath)\n",
    "            f.write(parsedPDF)\n",
    "            f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 마지막에 /를 붙이는 것이 포인트 -킹영창曰\n",
    "\n",
    "pdf2txt('/Users/seonmin/NLP_Hause/자연어처리/논문 자료수집/금통위 의사록 pdf/', '/Users/seonmin/NLP_Hause/자연어처리/논문 자료수집/금통위 의사록 텍스트/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개뚜래기 pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/pdfminer/pdfminer.six/issues/210\n",
    "https://github.com/pdfminer/pdfminer.six/pull/228\n",
    "    \n",
    "참고해서 본인 pdffond.py 파일 수정하고 진행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "import os\n",
    "\n",
    "\n",
    "def pdfparser(data):\n",
    "\n",
    "    fp = open(data, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    # Process each page contained in the document.\n",
    "\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "\n",
    "#     print(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     pdfparser(sys.argv[1])\n",
    "#     for i in pdf_list:\n",
    "    for i in pdf_list:\n",
    "        try:\n",
    "            with open(i + '.txt' , 'w', encoding='utf-8') as f:\n",
    "                a = pdfparser(i)\n",
    "                f.write(a)\n",
    "            oldname = i + '.txt'\n",
    "            newname = oldname[:-8]+'.txt'\n",
    "            os.rename(oldname, newname )\n",
    "        except:\n",
    "            print('오류: ',i)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 예외항목 자료 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://www.bok.or.kr/portal/bbs/B0000245/list.do?menuNo=200761&pageIndex=30\n",
    "    \n",
    "1. 위 사이트에서 의사록 목록 중 2005.05 이후부터의 파일 3개를 직접 다운받는다.\n",
    "게시글: 2005년도 제13차, 2005년도 제 12차, 2005년도 제 10차\n",
    "    * 2005년도 제 10차는 hwp파일만 있으므로 주의한다.\n",
    "\n",
    "2. 파일을 다운받으면 다음과 같은 이름으로 저장된다. \n",
    "2005년 제10차 금통위 의사록 # hwp파일\n",
    "2005년 제13차 금통위 의사록 - PDF파일\n",
    "2005년 제12차 금통위 의사록 - PDF파일\n",
    "\n",
    "# 3. 위에서 활용한 pdfminer로 2개 pdf 파일을 txt파일로 변환한다.\n",
    "\n",
    "4. 나머지 hwp파일은 직접 한글파일을 열어서 ctrl+a과 ctrl+c,v 활용하여 txt파일을 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
